{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522c36ef",
   "metadata": {},
   "source": [
    "* Write the parenthetic parse of the following sentence: (sentence will be provided - it will have several phrases, but will only have one clause).\n",
    "\n",
    "Ans: GPT it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843ada1",
   "metadata": {},
   "source": [
    "* Some languages (such as Mandarin, Japanese, and Yoruba) do not separate adjectives and verbs as clearly as English. Why might this pose a challenge for designing a POS tagset, especially given assumptions we've discussed in class.\n",
    "\n",
    "Ans: \n",
    "We are assuming that, first, it uses the standarized tagset, secondly, it classifies different word class into different groups, thirdly, it simply some rare tags so that it won't need to create new tag in the tagset.\n",
    "\n",
    "Since I'm Chinese, I will mainly use Mandarin as references.\n",
    "\n",
    "Some words in Mandarin breaks the first and second assumptions, for example: The maple leaves turn to red, in mandarin: '枫叶红了'， 枫叶 means maple leaves, where 红了 means turn into red. However, in mandarin, 红了 is both adjective and verb, it means the maple leaves turns to red and the red at the same. This violate the first assumption and second assumption, where we can not classify the word 红了 into different groups, if we do so, we will lose some information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cda4d4",
   "metadata": {},
   "source": [
    "* Up to this point, we've largely ignored function words, but they are extremely influential in parsing.  Give 2 reasons why.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Function words are indicator of the phrases. For example, 'The food I eat', where 'The' is a function word, and 'the food' is a noun phrase, where 'the' indicates that the noun phrase begins. Another example is the word 'will', by seeing will, we will know that it will be followed by a verb of base form, and they together will be a verb phrase. Thats why they are extremely influential in parsing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62048d6",
   "metadata": {},
   "source": [
    "* English is an SVO (Subject-Verb-Object) language, but only most of the time.  Can you think of an example where this order is violated?  Why do you think this doesn't confuse speakers of English?\n",
    "\n",
    "Ans:\n",
    "\n",
    "SVO: I like to read\n",
    "\n",
    "VSO: read, I like\n",
    "\n",
    "I think this doesn't confuse the spearker because we have the contextual and semantic information of the sentence, so our brain can automatically rearrange the words into plausible order so that we can understand them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7a1a3",
   "metadata": {},
   "source": [
    "* Why are treebanks expensive or difficult to create? Give two reasons.\n",
    "\n",
    "Ans: \n",
    "\n",
    "In the first place, treebank is a collection of sentences manually anootated for consituency trees. This is why it is expensive, it requires linguistic expertises to construct the treebanks, where these expertise needs to identify subtle grammar differences within thousands or even millions of sentences.\n",
    "\n",
    "Secondly, although the treebank follows a certain rules or guidelines, each expert has their own understanding to the same sentence, nad it will be highly difficult for every experts to agree on a ambiguous sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712f84b",
   "metadata": {},
   "source": [
    "* Imagine that two linguists are creating a treebank, but even though they have a clear annotation schema, they disagree on annotations about 10 percent of the time. How could you mitigate the effects of this disagreement on your downstream parser?\n",
    "\n",
    "Ans:\n",
    "\n",
    "My best suggestion is that every time they encounter a disagreement, they can discuss about it and add a new rule in the existing schema, and every time they encounter the same issue, they follow the rules set before. This not only save their time for debating with each other, but also create stable criterias during parsing so that the treebank is stable (although with some disagreement for some people)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd8aee",
   "metadata": {},
   "source": [
    "* You are building a parser for a language with much freer word order than English.  What assumptions do you need to weaken before building the parser.  Do you think it will have much of an impact on the quality of the parser?\n",
    "\n",
    "Ans:\n",
    "\n",
    "We discussed a language that the Noun phrase and Verb phrase can interset with each other in the class, so I think if we need to build parser based on these kind of languages, we need to weaken the linearity of the parser. The constituents may be non-consistent and we need to search the nearly the entire sentence to draw a tree like in English. However, we can draw dependency tree so that each word is a dependency of another one, this may help with the parsing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6deaadb",
   "metadata": {},
   "source": [
    "* You have a generative AI model that produces English text. How might you use a constituency parser to evaluate the quality of the text, and why would that be insufficient?  Give an example of one error that could be detected by the parser, and one that couldn't.\n",
    "\n",
    "Ans:\n",
    "\n",
    "LLM not only focus on the syntax information, but also relies on the semantics as well as pragmatics information. If we only parsing the texts with constituency parser, the model will be able to identify the syntax error, however, it will never learn the meaning of each word and corrsponding sentences it outputs. For example, \"I eat food yesterday\", which is correct because of the past tense, but \"Computer likes to jump\", this sentence is grammartically correct, but does not hold on the aspects of semantics and pragmatics, and the constituency parser will not be able to identify the correctness of such a sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60919398",
   "metadata": {},
   "source": [
    "* In class, every example we had was well-tokenized, but there are parsing cues within the shape of the word (its morphology).  Briefly explain how a parser could leverage this, with an example.\n",
    "\n",
    "Ans:\n",
    "\n",
    "The morphology contains some information of the sentence during parsing, such as the past tense of the word can indicate the advmod of the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb79873",
   "metadata": {},
   "source": [
    "* In dependency parsing, why might modifiers (like adjectives or adverbs) be easier to detect than obliques, and how does this relate to the chunking exercises we did in the lab?\n",
    "\n",
    "Ans:\n",
    "\n",
    "It is because modifiers are more likely to appear on hte left side of the noun, such as 'red apple', 'blue book', hence it is easier to detech adjectives or adverbs. However, obliques are more difficult to detect becasue the obliques can mean different meanings, such as 'I saw an elephant in my pajamas', the sentence can be interpreted as I wearing my pajams and saw an elephant, or I saw an elephant which wearing my pajamas. If we try to chunk it, we will get the result of [NP I] [VP saw] [NP an elephant] [PP in my pajamas], but we won't be able to know the dependcy of the PP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f29f5",
   "metadata": {},
   "source": [
    "* If you had a cascaded pipeline of constituency and dependency parsers, which would you run first?  What are the risks of getting it backwards?\n",
    "\n",
    "Ans:\n",
    "\n",
    "We need to run the constituency parser first then the dependency parser. It is because we need to obtain the information of the constituencies of the sentence since we can no longer obtain them if we parse it by dependency parser first.\n",
    "\n",
    "The dependency parser can only tells the relationships between words and words, it is unable to mark the whole NP, VP, PP, or other rules. If we run it backward, it will be extreme difficult to turn a flatten dependency list into a tree structure since the constituency parser won't be able to understand the output of the dependency parser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c2df5",
   "metadata": {},
   "source": [
    "* Why do we not use accuracy to evaluate chunkers?  Can you think of any other tasks where this might be as big (or bigger) of a problem?\n",
    "\n",
    "Ans:\n",
    "\n",
    "This is because of the class imbalance, 'O' is the most common label, so it is highly possible the accuracy directly depends on the counts of 'O's in the chunker of gold set and pred set, so we should use F1 score as the metrics to measure the predictions of the model to balance the recall and precision scores. The class imbalance is also a serious issue for, for example, disease check. It is possible that each person is 0.001% having this disease, a model with accuracy = 99.999% is high but we should focus on the precision score so that it can actually predict whether the person has the disease or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb3bc93",
   "metadata": {},
   "source": [
    "* What properties of English syntax make regular expressions suitable for chunking?  Do you think that this functionality would extend to many other languages?  Briefly explain.\n",
    "\n",
    "Ans:\n",
    "\n",
    "English syntax usually has fixed word order like DT -> JJ -> NN, meanwhile, the function words are also clear boundaries for the phrases, based on above, we can greedly search from left to right to match with the pattern. I don't think ths functionality would extend to many other languages because some languages like swiss german usually has intersect word order, where the noun phrases, verb phrases, or prepositional phrases may intersect with each other, which is difficult to tell the regex to find out a certain pattern with a fixed rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a729ec0",
   "metadata": {},
   "source": [
    "* Imagine you've been assigned the task of converting instructions in a recipe into a list of easy-to-accomplish goals for a cooking robot. How could you use a parser to aid your conversion?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Parser can help us to convert natural language to code. For example: \"fry the bacon for 5 minutes\", the parser will turn the natural language to (nn bacon) <-dobj (vb fry), then based on the parser, we can send the instruction fry(target = bacon, time = 5) to the robot. Similarily, if there are more instructions like using which pan or another action with a CC, we can turn them all from natural language to parameter and let the robot to do following actions based on the output of the parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac92949",
   "metadata": {},
   "source": [
    "* Imagine that you're working with a copy-editor to tighten the prose of prospective novels.  How might you use parsers to identify places where you can ``trim the fat'' without being too aggressive?\n",
    "\n",
    "Ans:\n",
    "\n",
    "The first case I can think of is the passive voice. I used to write a lot passive voice in my paper and nearly all of the grammar check will point it out. Passive voice uses 30% more vocabularies compare with active voice, so every time the parser detect a passive voice phrase, change it to the active voice. The second case is that, for example: \"The man is really tall\", \"The food is extremelly dilicious\", the advmods can usually be deleted safely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3a8e8",
   "metadata": {},
   "source": [
    "* When learning a language (whether an L1 or L2), speakers often make grammatical mistakes, but are still understandable by other speakers.  What do you think this says about the role of syntax in language, and how do you think it could help us create more robust language recognition systems?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Based on the fact, the syntax seems redundant. It is because we have contextual, semantics, and common sense information, although the syntax is wrong, we can still get what they are trying to say. If we want to enhance our language recognition system, we can stop restrict our rules to perfect but let them to find the most similar meanings if the sentence does not match with the rules. Furthermore, we need to add contextual or semantics information to our parser so that it can be able to obtain information from these dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f16e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
